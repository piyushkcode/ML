# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
df = pd.read_csv('diabetes.csv')  # Ensure correct path to dataset

# Check the data structure
print(df.info())
print(df.head())

# Step 1: Data Preprocessing (Check for missing values)
print("Missing values in each column:\n", df.isnull().sum())

# We assume there are no missing values here; otherwise, handle them accordingly.

# Step 2: Selecting one feature to predict 'Outcome'
# Here we select 'BMI' to predict 'Outcome'
X = df[['BMI']].values  # Independent variable (using double brackets to get a 2D array for sklearn)
y = df['Outcome'].values  # Dependent variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 3: Regression Results
# Calculate the coefficient and intercept
print("Coefficient:", model.coef_[0])
print("Intercept:", model.intercept_)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate the Residual Sum of Squares (RSS)
rss = np.sum((y_test - y_pred) ** 2)
print("Residual Sum of Squares (RSS):", rss)

# Calculate the Coefficient of Determination (R-squared)
r_squared = r2_score(y_test, y_pred)
print("Coefficient of Determination (R-squared):", r_squared)

# Step 4: Plotting the Regression Line
plt.figure(figsize=(8, 5))
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')
plt.xlabel('BMI')
plt.ylabel('Outcome')
plt.title('Regression Line - BMI vs Diabetes Outcome')
plt.legend()
plt.show()
